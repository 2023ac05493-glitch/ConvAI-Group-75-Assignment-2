{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf0f72ad",
   "metadata": {},
   "source": [
    "Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8df1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be0ca6",
   "metadata": {},
   "source": [
    "Load the Question-Answer pair json and convert to Prompt-Completion pair json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d831a092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 100 pairs.\n",
      "Saved GPT-style fine-tuning dataset to: ../data\\qa_pairs_ft_100.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ROOT_DIR = \"../\"   # adjust if needed (depending on where the notebook lives)\n",
    "\n",
    "# Load the 50 sampled Q/A pairs\n",
    "qa_path = os.path.join(ROOT_DIR, \"data\", \"qa_pairs_100.json\")\n",
    "with open(qa_path, \"r\") as f:\n",
    "    qa_pairs = json.load(f)\n",
    "\n",
    "# Convert to GPT prompt/completion format\n",
    "gpt_ft_pairs = []\n",
    "for item in qa_pairs:\n",
    "    gpt_ft_pairs.append({\n",
    "        \"prompt\":  item[\"question\"],\n",
    "        \"completion\": item[\"answer\"]\n",
    "    })\n",
    "\n",
    "print(f\"Converted {len(gpt_ft_pairs)} pairs.\")\n",
    "\n",
    "# Save to a new file for fine-tuning\n",
    "ft_path = os.path.join(ROOT_DIR, \"data\", \"qa_pairs_ft_100.json\")\n",
    "with open(ft_path, \"w\") as f:\n",
    "    json.dump(gpt_ft_pairs, f, indent=2)\n",
    "\n",
    "print(f\"Saved GPT-style fine-tuning dataset to: {ft_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555915d0",
   "metadata": {},
   "source": [
    "Load the model (distilgpt2) and get the baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ea8f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': \"What was BERKSHIRE HATHAWAY INC's total liabilities in 20221231?\",\n",
       "  'generated_answer': \"What was BERKSHIRE HATHAWAY INC's total liabilities in 20221231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'BERKSHIRE HATHAWAY INC reported total liabilities of 466784000000.0 in 20221231.',\n",
       "  'response_time_sec': 2.448549747467041},\n",
       " {'question': \"What was FIDELITY NATIONAL INFORMATION SERVICES, INC.'s cash and cash equivalents in 20241231?\",\n",
       "  'generated_answer': \"What was FIDELITY NATIONAL INFORMATION SERVICES, INC.'s cash and cash equivalents in 20241231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'FIDELITY NATIONAL INFORMATION SERVICES, INC. reported cash and cash equivalents of 834000000.0 in 20241231.',\n",
       "  'response_time_sec': 2.61784029006958},\n",
       " {'question': \"What was GLOBAL GAS CORP's net income in 20231231?\",\n",
       "  'generated_answer': \"What was GLOBAL GAS CORP's net income in 20231231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'GLOBAL GAS CORP reported net income of -300176.0 in 20231231.',\n",
       "  'response_time_sec': 2.2638115882873535},\n",
       " {'question': \"What was YUM BRANDS INC's revenue in 20241231?\",\n",
       "  'generated_answer': \"What was YUM BRANDS INC's revenue in 20241231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'YUM BRANDS INC reported revenue of 7549000000.0 in 20241231.',\n",
       "  'response_time_sec': 2.3063364028930664},\n",
       " {'question': \"What was REINSURANCE GROUP OF AMERICA INC's total liabilities in 20211231?\",\n",
       "  'generated_answer': \"What was REINSURANCE GROUP OF AMERICA INC's total liabilities in 20211231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'REINSURANCE GROUP OF AMERICA INC reported total liabilities of 84761000000.0 in 20211231.',\n",
       "  'response_time_sec': 2.41894268989563},\n",
       " {'question': \"What was PCS EDVENTURES!, INC.'s cash and cash equivalents in 20230331?\",\n",
       "  'generated_answer': \"What was PCS EDVENTURES!, INC.'s cash and cash equivalents in 20230331?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'PCS EDVENTURES!, INC. reported cash and cash equivalents of 442657.0 in 20230331.',\n",
       "  'response_time_sec': 2.4311270713806152},\n",
       " {'question': \"What was EQUITABLE FINANCIAL LIFE INSURANCE CO's revenue in 20221231?\",\n",
       "  'generated_answer': \"What was EQUITABLE FINANCIAL LIFE INSURANCE CO's revenue in 20221231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'EQUITABLE FINANCIAL LIFE INSURANCE CO reported revenue of 0.0 in 20221231.',\n",
       "  'response_time_sec': 2.6397175788879395},\n",
       " {'question': \"What was IIOT-OXYS, INC.'s net income in 20231231?\",\n",
       "  'generated_answer': \"What was IIOT-OXYS, INC.'s net income in 20231231?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'IIOT-OXYS, INC. reported net income of -1067929.0 in 20231231.',\n",
       "  'response_time_sec': 2.4720423221588135},\n",
       " {'question': \"What was JANEL CORP's total assets in 20230930?\",\n",
       "  'generated_answer': \"What was JANEL CORP's total assets in 20230930?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'JANEL CORP reported total assets of 34823000.0 in 20230930.',\n",
       "  'response_time_sec': 2.62925124168396},\n",
       " {'question': \"What was GENESCO INC's cash and cash equivalents in 20240131?\",\n",
       "  'generated_answer': \"What was GENESCO INC's cash and cash equivalents in 20240131?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  'ground_truth': 'GENESCO INC reported cash and cash equivalents of 23200000.0 in 20240131.',\n",
       "  'response_time_sec': 2.2086687088012695}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load GPT-style Q/A data (from qa_pairs_ft_5000.json)\n",
    "ft_path = os.path.join(ROOT_DIR, \"data\", \"qa_pairs_ft_100.json\")\n",
    "with open(ft_path, \"r\") as f:\n",
    "    gpt_pairs = json.load(f)\n",
    "\n",
    "# Take first 10 questions as baseline test\n",
    "baseline_questions = gpt_pairs[:10]\n",
    "\n",
    "# Load pre-trained distilgpt2\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "# Evaluate\n",
    "baseline_results = []\n",
    "for item in baseline_questions:\n",
    "    input_prompt = item[\"prompt\"]\n",
    "    gt_answer = item[\"completion\"]\n",
    "    \n",
    "    start = time.time()\n",
    "    inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=50)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    baseline_results.append({\n",
    "        \"question\": input_prompt,\n",
    "        \"generated_answer\": answer,\n",
    "        \"ground_truth\": gt_answer,\n",
    "        \"response_time_sec\": elapsed\n",
    "    })\n",
    "\n",
    "baseline_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c467d8a",
   "metadata": {},
   "source": [
    "Fine-tune the model over the Prompt-Completion 100 pairs json dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual fine-tuning of distilgpt2 (CPU friendly)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load the prompt/completion pairs\n",
    "ft_path = os.path.join(ROOT_DIR, \"data\", \"qa_pairs_ft_100.json\")\n",
    "with open(ft_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load model + tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# ---- PAD TOKEN FIX ----\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 4\n",
    "learning_rate = 5e-5\n",
    "\n",
    "# Log hyperparameters and setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"🧠 Fine-tuning configuration:\")\n",
    "print(f\"• Model: {model_name}\")\n",
    "print(f\"• Learning rate: {learning_rate}\")\n",
    "print(f\"• Batch size: 1 (manual, per sample)\")\n",
    "print(f\"• Number of epochs: {num_epochs}\")\n",
    "print(f\"• Compute device: {device}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for entry in data:\n",
    "        prompt = entry[\"prompt\"]\n",
    "        completion = entry[\"completion\"]\n",
    "        text = prompt + \"\\n\" + completion\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=\"max_length\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}  |  avg loss = {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb505b",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb369dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete.\n",
      "Model saved to: ../models/fine_tuned_model\n"
     ]
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "save_path = \"../models/fine_tuned_model\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Fine-tuning complete.\")\n",
    "print(f\"Model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c054f83",
   "metadata": {},
   "source": [
    "Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b048c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was BERKSHIRE HATHAWAY INC's total liabilities in 20221231?\n",
      "Generated: BERKSHIRE HATHAWAY INC reported total liabilities of -0.0 in 20221231.\n",
      "Ground truth: BERKSHIRE HATHAWAY INC reported total liabilities of 466784000000.0 in 20221231.\n",
      "Time: 1.13s\n",
      "------------------------------------------------------------\n",
      "Q: What was FIDELITY NATIONAL INFORMATION SERVICES, INC.'s cash and cash equivalents in 20241231?\n",
      "Generated: FIDELITY NATIONAL INFORMATION SERVICES, INC. reported cash and cash equivalents of -0.0 in 20241231.\n",
      "Ground truth: FIDELITY NATIONAL INFORMATION SERVICES, INC. reported cash and cash equivalents of 834000000.0 in 20241231.\n",
      "Time: 1.02s\n",
      "------------------------------------------------------------\n",
      "Q: What was GLOBAL GAS CORP's net income in 20231231?\n",
      "Generated: GLOBAL GAS CORP reported net income of -0.0 in 20231231.\n",
      "Ground truth: GLOBAL GAS CORP reported net income of -300176.0 in 20231231.\n",
      "Time: 0.99s\n",
      "------------------------------------------------------------\n",
      "Q: What was YUM BRANDS INC's revenue in 20241231?\n",
      "Generated: YUM BRANDS INC reported revenue of -0.0 in 20241231.\n",
      "Ground truth: YUM BRANDS INC reported revenue of 7549000000.0 in 20241231.\n",
      "Time: 0.91s\n",
      "------------------------------------------------------------\n",
      "Q: What was REINSURANCE GROUP OF AMERICA INC's total liabilities in 20211231?\n",
      "Generated: REINSURANCE GROUP OF AMERICA INC reported total liabilities of -0.0 in 20211231.\n",
      "Ground truth: REINSURANCE GROUP OF AMERICA INC reported total liabilities of 84761000000.0 in 20211231.\n",
      "Time: 1.05s\n",
      "------------------------------------------------------------\n",
      "Q: What was PCS EDVENTURES!, INC.'s cash and cash equivalents in 20230331?\n",
      "Generated: PCS EDVENTURES!, INC. reported cash and cash equivalents of -0.0 in 20230331.\n",
      "Ground truth: PCS EDVENTURES!, INC. reported cash and cash equivalents of 442657.0 in 20230331.\n",
      "Time: 0.97s\n",
      "------------------------------------------------------------\n",
      "Q: What was EQUITABLE FINANCIAL LIFE INSURANCE CO's revenue in 20221231?\n",
      "Generated: EquitABLE FINANCIAL LIFE INSURANCE CO reported revenue of -0.0 in 20221231.\n",
      "Ground truth: EQUITABLE FINANCIAL LIFE INSURANCE CO reported revenue of 0.0 in 20221231.\n",
      "Time: 1.09s\n",
      "------------------------------------------------------------\n",
      "Q: What was IIOT-OXYS, INC.'s net income in 20231231?\n",
      "Generated: IIOT-OXYS, INC. reported net income of -0.0 in 20231231.\n",
      "Ground truth: IIOT-OXYS, INC. reported net income of -1067929.0 in 20231231.\n",
      "Time: 1.52s\n",
      "------------------------------------------------------------\n",
      "Q: What was JANEL CORP's total assets in 20230930?\n",
      "Generated: JANEL CORP reported total assets of 547000000.0 in 20230930.\n",
      "Ground truth: JANEL CORP reported total assets of 34823000.0 in 20230930.\n",
      "Time: 1.48s\n",
      "------------------------------------------------------------\n",
      "Q: What was GENESCO INC's cash and cash equivalents in 20240131?\n",
      "Generated: GENESCO INC reported cash and cash equivalents of -0.0 in 20240131.\n",
      "Ground truth: GENESCO INC reported cash and cash equivalents of 23200000.0 in 20240131.\n",
      "Time: 1.42s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model on a few questions\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, json, time\n",
    "\n",
    "# Load fine-tuned model\n",
    "ft_model_path = \"../models/fine_tuned_model\"\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(ft_model_path)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(ft_model_path)\n",
    "\n",
    "# Load the same 10 test questions we used before\n",
    "with open(os.path.join(ROOT_DIR, \"data\", \"qa_pairs_ft_100.json\"), \"r\") as f:\n",
    "    qa_full = json.load(f)\n",
    "\n",
    "test_examples = qa_full[:10]\n",
    "\n",
    "model_ft.eval()\n",
    "for item in test_examples:\n",
    "    prompt = item[\"prompt\"]\n",
    "    gt_answer = item[\"completion\"]\n",
    "    \n",
    "    start = time.time()\n",
    "    # Encode the prompt and attention mask\n",
    "    inputs = tokenizer_ft(prompt, return_tensors=\"pt\", padding=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Generate only the answer (don't repeat prompt)\n",
    "    outputs = model_ft.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer_ft.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Remove the prompt part from the output\n",
    "    generated_text = tokenizer_ft.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated_text.replace(prompt, \"\").strip()\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\"Q:\", prompt)\n",
    "    print(\"Generated:\", answer)\n",
    "    print(\"Ground truth:\", gt_answer)\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcad90f",
   "metadata": {},
   "source": [
    "Apply Continual Learning/ Domain Adaptation technique for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2760c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== Continual Learning / Domain Adaptation (Step 3.5) ===============\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the already fine-tuned model\n",
    "ft_model_path = \"../models/fine_tuned_model\"\n",
    "model = AutoModelForCausalLM.from_pretrained(ft_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ft_model_path)\n",
    "\n",
    "# Load financial_sentences.csv to use as domain-adaptation text\n",
    "csv_path = os.path.join(ROOT_DIR, \"data\", \"processed\", \"financial_sentences.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Optional: sample ~200 sentences (small continual-learning batch)\n",
    "domain_sentences = df['sentence'].drop_duplicates().sample(200, random_state=42).tolist()\n",
    "\n",
    "# Convert to prompt/completion pairs (self-supervised style)\n",
    "domain_pairs = [{\"prompt\": s, \"completion\": s} for s in domain_sentences]\n",
    "\n",
    "# Second training pass (few epochs)\n",
    "num_epochs = 2\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for entry in domain_pairs:\n",
    "        text = entry[\"prompt\"] + \"\\n\" + entry[\"completion\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256, padding=\"max_length\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(domain_pairs)\n",
    "    print(f\"[Continual Phase] Epoch {epoch+1}/{num_epochs} | avg loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4dac9",
   "metadata": {},
   "source": [
    "Save the updated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40dab894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Continual learning stage complete (model updated).\n"
     ]
    }
   ],
   "source": [
    "# Save updated model\n",
    "cont_path = \"../models/fine_tuned_model\"  # overwrite previous version\n",
    "model.save_pretrained(cont_path)\n",
    "tokenizer.save_pretrained(cont_path)\n",
    "\n",
    "print(\"✅ Continual learning stage complete (model updated).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f952974d",
   "metadata": {},
   "source": [
    "Implement Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f1c1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Guardrail (Input Filtering) ------------------------\n",
    "\n",
    "# We define a basic list of FINANCIAL keywords\n",
    "FINANCIAL_KEYWORDS = [\n",
    "    \"revenue\", \"net income\", \"liabilities\", \"assets\", \"cash\", \"equivalents\",\n",
    "    \"earnings\", \"profit\", \"income\", \"expenses\"\n",
    "]\n",
    "\n",
    "def is_financial_question(question: str) -> bool:\n",
    "    \"\"\" Returns True if the question contains any financial keyword. \"\"\"\n",
    "    q_lower = question.lower()\n",
    "    for kw in FINANCIAL_KEYWORDS:\n",
    "        if kw in q_lower:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Example usage in your inference function\n",
    "def generate_answer_with_guardrail(question: str, model, tokenizer):\n",
    "    # Guardrail check\n",
    "    if not is_financial_question(question):\n",
    "        return \"[Input Guardrail] Sorry, I can only answer financial-statement-related questions.\"\n",
    "    \n",
    "    # Normal generation\n",
    "    inputs = tokenizer.encode(question, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_new_tokens=50)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef623697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input Guardrail] Sorry, I can only answer financial-statement-related questions.\n",
      "What was Amazon's revenue in 2023?\n",
      "Amazon reported revenue of -0.0 in 2023.\n"
     ]
    }
   ],
   "source": [
    "print( generate_answer_with_guardrail(\"What is the capital of France?\", model_ft, tokenizer_ft) )\n",
    "\n",
    "print( generate_answer_with_guardrail(\"What was Amazon's revenue in 2023?\", model_ft, tokenizer_ft) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b79ee",
   "metadata": {},
   "source": [
    "Confidence score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c129f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_confidence(prompt, model, tokenizer, max_new_tokens=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # Generate output with scores\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Extract generated tokens (excluding the prompt)\n",
    "    generated_tokens = outputs.sequences[0][input_ids.shape[-1]:]\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    # Get token-level probabilities\n",
    "    scores = outputs.scores  # List of logits per token\n",
    "    token_confidences = []\n",
    "    \n",
    "    for i, (score, token_id) in enumerate(zip(scores, generated_tokens)):\n",
    "        try:\n",
    "            # Get the probability distribution for this token position\n",
    "            probs = F.softmax(score, dim=-1)\n",
    "            # Get the probability of the generated token\n",
    "            token_prob = probs[0, token_id].item()\n",
    "            token_confidences.append(token_prob)\n",
    "        except IndexError:\n",
    "            # If there's an index error, skip this token\n",
    "            token_confidences.append(0.0)\n",
    "            continue\n",
    "\n",
    "    avg_confidence = sum(token_confidences) / len(token_confidences) if token_confidences else 0.0\n",
    "\n",
    "    return {\n",
    "        \"generated_text\": generated_text.strip(),\n",
    "        \"token_confidences\": token_confidences,\n",
    "        \"avg_confidence\": avg_confidence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aba109",
   "metadata": {},
   "source": [
    "Test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d925af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What was BERKSHIRE HATHAWAY INC's total liabilities in 20221231?\n",
      "Generated: BERKSHIRE HATHAWAY INC reported total liabilities of 101000000.0 in 20221231.\n",
      "Ground Truth: BERKSHIRE HATHAWAY INC reported total liabilities of 466784000000.0 in 20221231.\n",
      "Confidence: 0.896\n",
      "Time: 1.04s\n",
      "------------------------------------------------------------\n",
      "Q: What was FIDELITY NATIONAL INFORMATION SERVICES, INC.'s cash and cash equivalents in 20241231?\n",
      "Generated: FIDELITY NATIONAL INFORMATION SERVICES, INC. reported cash and cash equivalents of 20241231.\n",
      "Ground Truth: FIDELITY NATIONAL INFORMATION SERVICES, INC. reported cash and cash equivalents of 834000000.0 in 20241231.\n",
      "Confidence: 0.978\n",
      "Time: 1.08s\n",
      "------------------------------------------------------------\n",
      "Q: What was GLOBAL GAS CORP's net income in 20231231?\n",
      "Generated: GLOBAL GAS CORP reported net income of -0.0 in 20231231.\n",
      "Ground Truth: GLOBAL GAS CORP reported net income of -300176.0 in 20231231.\n",
      "Confidence: 0.910\n",
      "Time: 1.08s\n",
      "------------------------------------------------------------\n",
      "Q: What was YUM BRANDS INC's revenue in 20241231?\n",
      "Generated: YUM BRANDS INC reported revenue of 101000000.0 in 20241231.\n",
      "Ground Truth: YUM BRANDS INC reported revenue of 7549000000.0 in 20241231.\n",
      "Confidence: 0.886\n",
      "Time: 0.87s\n",
      "------------------------------------------------------------\n",
      "Q: What was REINSURANCE GROUP OF AMERICA INC's total liabilities in 20211231?\n",
      "Generated: REINSURANCE GROUP OF AMERICA INC reported total liabilities of 101000000.0 in 20211231.\n",
      "Ground Truth: REINSURANCE GROUP OF AMERICA INC reported total liabilities of 84761000000.0 in 20211231.\n",
      "Confidence: 0.915\n",
      "Time: 1.00s\n",
      "------------------------------------------------------------\n",
      "Q: What was PCS EDVENTURES!, INC.'s cash and cash equivalents in 20230331?\n",
      "Generated: PCS EDVENTURES!, INC. reported cash and cash equivalents of 20230331.\n",
      "Ground Truth: PCS EDVENTURES!, INC. reported cash and cash equivalents of 442657.0 in 20230331.\n",
      "Confidence: 0.938\n",
      "Time: 0.84s\n",
      "------------------------------------------------------------\n",
      "Q: What was EQUITABLE FINANCIAL LIFE INSURANCE CO's revenue in 20221231?\n",
      "Generated: EVITABLE FINANCIAL LIFE INSURANCE CO reported revenue of -117000000.0 in 20221231.\n",
      "Ground Truth: EQUITABLE FINANCIAL LIFE INSURANCE CO reported revenue of 0.0 in 20221231.\n",
      "Confidence: 0.864\n",
      "Time: 1.52s\n",
      "------------------------------------------------------------\n",
      "Q: What was IIOT-OXYS, INC.'s net income in 20231231?\n",
      "Generated: IIOT-OXYS, INC. reported net income of -0.0 in 20231231.\n",
      "Ground Truth: IIOT-OXYS, INC. reported net income of -1067929.0 in 20231231.\n",
      "Confidence: 0.926\n",
      "Time: 1.46s\n",
      "------------------------------------------------------------\n",
      "Q: What was JANEL CORP's total assets in 20230930?\n",
      "Generated: JANEL CORP reported total assets of 230930.0 in 20230930.\n",
      "Ground Truth: JANEL CORP reported total assets of 34823000.0 in 20230930.\n",
      "Confidence: 0.935\n",
      "Time: 1.16s\n",
      "------------------------------------------------------------\n",
      "Q: What was GENESCO INC's cash and cash equivalents in 20240131?\n",
      "Generated: GENESCO INC reported cash and cash equivalents of 20240131.\n",
      "Ground Truth: GENESCO INC reported cash and cash equivalents of 23200000.0 in 20240131.\n",
      "Confidence: 0.909\n",
      "Time: 0.90s\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Test the fine-tuned model on a few questions\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, json, time\n",
    "\n",
    "# Load fine-tuned model\n",
    "ft_model_path = \"../models/fine_tuned_model\"\n",
    "model_ft = AutoModelForCausalLM.from_pretrained(ft_model_path)\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(ft_model_path)\n",
    "\n",
    "# Load the same 10 test questions we used before\n",
    "with open(os.path.join(ROOT_DIR, \"data\", \"qa_pairs_ft_100.json\"), \"r\") as f:\n",
    "    qa_full = json.load(f)\n",
    "\n",
    "test_examples = qa_full[:10]\n",
    "\n",
    "model_ft.eval()\n",
    "for item in test_examples:\n",
    "    prompt = item[\"prompt\"]\n",
    "    gt_answer = item[\"completion\"]\n",
    "\n",
    "    start = time.time()\n",
    "    result = generate_with_confidence(prompt, model_ft, tokenizer_ft)\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    print(\"Q:\", prompt)\n",
    "    print(\"Generated:\", result[\"generated_text\"])\n",
    "    print(\"Ground Truth:\", gt_answer)\n",
    "    print(f\"Confidence: {result['avg_confidence']:.3f}\")\n",
    "    print(f\"Time: {elapsed:.2f}s\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d0a319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
