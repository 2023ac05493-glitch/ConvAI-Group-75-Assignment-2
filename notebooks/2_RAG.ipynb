{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dcb8cc1",
   "metadata": {},
   "source": [
    "Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e8d25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbdff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_answer(query, retrieved_chunks):\n",
    "    \"\"\"\n",
    "    Attempt to extract a numeric answer from retrieved sentences\n",
    "    by matching relevant financial keywords in the query.\n",
    "    \"\"\"\n",
    "    # Define common financial metric keywords\n",
    "    keywords = {\n",
    "        'revenue': ['revenue', 'revenues', 'sales'],\n",
    "        'net income': ['net income', 'income', 'net earnings', 'profit'],\n",
    "        'assets': ['total assets', 'assets'],\n",
    "        'liabilities': ['total liabilities', 'liabilities'],\n",
    "        'cash': ['cash and cash equivalents', 'cash']\n",
    "    }\n",
    "\n",
    "    # Normalize query\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Find which category the query is targeting\n",
    "    target_category = None\n",
    "    for key, terms in keywords.items():\n",
    "        if any(term in query_lower for term in terms):\n",
    "            target_category = key\n",
    "            break\n",
    "\n",
    "    if not target_category:\n",
    "        return None  # No match found\n",
    "\n",
    "    # Search for numbers in relevant retrieved chunks\n",
    "    for chunk in retrieved_chunks:\n",
    "        text = chunk['text']\n",
    "        if any(term in text.lower() for term in keywords[target_category]):\n",
    "            # Find number in sentence\n",
    "            match = re.search(r'([\\-]?\\d[\\d,]*\\.?\\d*)', text)\n",
    "            if match:\n",
    "                num_str = match.group(1).replace(',', '')\n",
    "                try:\n",
    "                    value = float(num_str)\n",
    "                    return value\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    return None  # No numeric value confidently extracted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f9e73",
   "metadata": {},
   "source": [
    "Load the tokenizer and the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1d05da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and generative model\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf595d",
   "metadata": {},
   "source": [
    "Load the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f26e392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_path = \"../data/processed/financial_sentences_10k.xlsx\"\n",
    "df = pd.read_excel(data_path)\n",
    "text_col = 'sentence'\n",
    "sentences = df[text_col].dropna().astype(str).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8b0a77",
   "metadata": {},
   "source": [
    "Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4362f4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Sentence-level Chunking ----\n",
    "def sentence_chunking(sentence_list):\n",
    "    return [sent.strip() for sent in sentence_list if len(sent.strip()) > 0]\n",
    "\n",
    "chunks = sentence_chunking(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af2bce",
   "metadata": {},
   "source": [
    "Loading embedding model and the cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dca6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model (E5-small-v2) and the cross encoder model...\n"
     ]
    }
   ],
   "source": [
    "# ---- Build BM25 + FAISS on sentence-level chunks ----\n",
    "print(\"Loading embedding model (E5-small-v2) and the cross encoder model...\")\n",
    "embedding_model = SentenceTransformer('intfloat/e5-small-v2')\n",
    "\n",
    "# Load cross-encoder for multi-stage reranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b18f3",
   "metadata": {},
   "source": [
    "Encoding the chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcafd459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding sentence chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c20e738dabc46bc8ede1abe5f06180f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BM25 index\n",
    "tokenized_chunks = [tokenizer.tokenize(chunk.lower()) for chunk in chunks]\n",
    "bm25 = BM25Okapi(tokenized_chunks)\n",
    "# FAISS index\n",
    "print(\"Encoding sentence chunks...\")\n",
    "embeddings = embedding_model.encode(chunks, show_progress_bar=True)\n",
    "embeddings = np.array(embeddings, dtype=np.float32)\n",
    "dimension = embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "faiss_index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090cbb5",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0ee7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save indices and chunks for Streamlit usage\n",
    "output_dir = \"../models/rag_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "faiss.write_index(faiss_index, f\"{output_dir}/faiss_index.bin\")\n",
    "with open(f\"{output_dir}/bm25_index.pkl\", 'wb') as f: pickle.dump(bm25, f)\n",
    "with open(f\"{output_dir}/chunks.json\", 'w') as f: json.dump(chunks, f)\n",
    "np.save(f\"{output_dir}/embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2ac33",
   "metadata": {},
   "source": [
    "Preprocess the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc27c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Preprocessing ----\n",
    "def preprocess_query(query: str) -> str:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    q = re.sub(r'[^a-zA-Z0-9\\s]', '', query.lower())\n",
    "    tokens = tokenizer.tokenize(q)\n",
    "    tokens = [t.lstrip('Ä ') for t in tokens]\n",
    "    filtered = [t for t in tokens if t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8128db77",
   "metadata": {},
   "source": [
    "Retrieve the chunk based on semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80eda9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Retrieval ----\n",
    "def dense_retrieval(query, top_k):\n",
    "    q_emb = embedding_model.encode([query])\n",
    "    q_emb = np.array(q_emb, dtype=np.float32)\n",
    "    dists, ids = faiss_index.search(q_emb, top_k)\n",
    "    results = []\n",
    "    for dist, idx in zip(dists[0], ids[0]):\n",
    "        if idx != -1:\n",
    "            results.append({'chunk_id': int(idx),'text': chunks[idx],'score': float(1/(1+dist)),'method':'dense'})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bfbf6",
   "metadata": {},
   "source": [
    "Retrieve the chunk based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "477d7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_retrieval(query, top_k):\n",
    "    tokens = preprocess_query(query).split()\n",
    "    scores = bm25.get_scores(tokens)\n",
    "    top_idxs = np.argsort(scores)[::-1][:top_k]\n",
    "    results = []\n",
    "    for i in top_idxs:\n",
    "        if scores[i] > 0:\n",
    "            results.append({'chunk_id':int(i),'text': chunks[i],'score':float(scores[i]),'method':'sparse'})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59b0a932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_results(dense, sparse, alpha=0.7):\n",
    "    combined = {}\n",
    "    if dense:\n",
    "        maxd = max(r['score'] for r in dense)\n",
    "        for r in dense: r['score'] /= maxd\n",
    "    if sparse:\n",
    "        maxs = max(r['score'] for r in sparse)\n",
    "        for r in sparse: r['score'] /= maxs\n",
    "    for r in dense: combined[r['chunk_id']]={'chunk_id':r['chunk_id'],'text':r['text'],'score':alpha*r['score'],'method':'dense'}\n",
    "    for r in sparse:\n",
    "        if r['chunk_id'] in combined:\n",
    "            combined[r['chunk_id']]['score']+= (1-alpha)*r['score']\n",
    "            combined[r['chunk_id']]['method']='hybrid'\n",
    "        else:\n",
    "            combined[r['chunk_id']]={'chunk_id':r['chunk_id'],'text':r['text'],'score':(1-alpha)*r['score'],'method':'sparse'}\n",
    "    return sorted(combined.values(), key=lambda x:x['score'], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56cec9f",
   "metadata": {},
   "source": [
    "Guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6da017c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Guardrails ----\n",
    "class RAGGuardrails:\n",
    "    def __init__(self):\n",
    "        self.harmful_patterns=[r'\\b(hack|steal|illegal|fraud|scam)\\b']\n",
    "        self.min_query_length=3\n",
    "    def validate_input(self,q):\n",
    "        if len(q.strip())<3: return False, 'Query too short'\n",
    "        return True,'ok'\n",
    "    def validate_output(self,rtext):\n",
    "        return True,'ok'\n",
    "\n",
    "guardrails = RAGGuardrails()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a83e8a",
   "metadata": {},
   "source": [
    "RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cb90daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Main Pipeline with metadata filtering ----\n",
    "def rag_pipeline(query, top_k=5):\n",
    "    \"\"\"Main RAG pipeline with multi-stage retrieval\"\"\"\n",
    "    ok,msg = guardrails.validate_input(query)\n",
    "    if not ok:\n",
    "        return {'query':query,'answer':msg,'retrieved_chunks':[], 'confidence':0.0,'method':'validation'}\n",
    "\n",
    "    q_lower = query.lower()\n",
    "    company_candidates = [name for name in df['name'].unique() if name.lower() in q_lower]\n",
    "    year_candidates = re.findall(r'20\\d{2}', query)\n",
    "    filtered_df = df.copy()\n",
    "    if company_candidates:\n",
    "        filtered_df = filtered_df[filtered_df['name'].str.lower().isin([c.lower() for c in company_candidates])]\n",
    "    if year_candidates:\n",
    "        yr = year_candidates[0]\n",
    "        filtered_df = filtered_df[ filtered_df['ddate'].astype(str).str.startswith(yr) ]\n",
    "    candidate_sentences = filtered_df[text_col].dropna().astype(str).tolist()\n",
    "    search_chunks = sentence_chunking(candidate_sentences) if candidate_sentences else chunks\n",
    "\n",
    "    processed = preprocess_query(query)\n",
    "    dense = dense_retrieval(processed, top_k)\n",
    "    sparse = sparse_retrieval(processed, top_k)\n",
    "    # Stage1 hybrid retrieval\n",
    "    stage1 = combine_results(dense, sparse)[:20]\n",
    "    # Stage2 reranking using cross-encoder\n",
    "    texts_for_rerank = [r['text'] for r in stage1]\n",
    "    cross_scores = cross_encoder.predict([(query, t) for t in texts_for_rerank])\n",
    "    for i,r in enumerate(stage1): r['final_score'] = 0.5*r['score'] + 0.5*float(cross_scores[i])\n",
    "    retrieved = sorted(stage1, key=lambda x: x['final_score'], reverse=True)[:top_k]\n",
    "\n",
    "\n",
    "    context = \" \".join([c['text'] for c in retrieved])\n",
    "    prompt = f\"Answer the question based on the context below.\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "    numeric_answer = extract_numeric_answer(query, retrieved)\n",
    "\n",
    "    if numeric_answer is not None:\n",
    "        answer = f\"{numeric_answer: ,.0f}\"\n",
    "    else:\n",
    "        # fallback LM answer\n",
    "        if context.strip():  # only call generator if there is context\n",
    "            resp = generator(prompt, max_new_tokens=80, do_sample=False)\n",
    "            raw_text = resp[0]['generated_text']\n",
    "            answer = raw_text.split('Answer:')[-1].split('\\n')[0].strip()\n",
    "        else:\n",
    "            answer = \"No relevant information found in the data.\"\n",
    "\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'retrieved_chunks': retrieved,\n",
    "        'confidence': np.mean([r['score'] for r in retrieved]) if retrieved else 0.0,\n",
    "        'method': 'rag_numeric'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5c37741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What was MR. COOPER GROUP INC's revenue in 2023?\n",
      "Answer:  2,464,000,000\n",
      "Confidence: 0.277\n",
      "\n",
      "Query: How much net income did PITNEY BOWES INC report in 2022?\n",
      "Answer: -6000.0\n",
      "Confidence: 0.28\n",
      "\n",
      "Query: What were the total assets of CMS ENERGY CORP in 2022?\n",
      "Answer:  1,102,000,000\n",
      "Confidence: 0.279\n",
      "\n",
      "Query: What is the capital of France?\n",
      "Answer: ALTISOURCE PORTFOLIO SOLUTIONS S.A.\n",
      "Confidence: 0.697\n"
     ]
    }
   ],
   "source": [
    "# --------------------------\n",
    "# Quick sanity check\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sample_queries = [\n",
    "        \"What was MR. COOPER GROUP INC's revenue in 2023?\",\n",
    "        \"How much net income did PITNEY BOWES INC report in 2022?\",\n",
    "        \"What were the total assets of CMS ENERGY CORP in 2022?\",\n",
    "        \"What is the capital of France?\"\n",
    "    ]\n",
    "\n",
    "    for q in sample_queries:\n",
    "        result = rag_pipeline(q, top_k=5)\n",
    "        print(\"\\nQuery:\", q)\n",
    "        print(\"Answer:\", result['answer'])\n",
    "        print(\"Confidence:\", round(result['confidence'],3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc06d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
